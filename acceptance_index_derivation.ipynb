{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from libs.usage_examples import get_acceptance_indexes\n",
    "from libs.noise_filter import NoiseFilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_COL = '2_way_label'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data\"\n",
    "# data_path = os.path.join(data_dir, 'merged_cleaned_data_v26_NoImage.tsv')\n",
    "# df = pd.read_csv(data_path, sep='\\t')\n",
    "data_path = os.path.join(data_dir, 'merged_cleaned_data_v30_news.csv')\n",
    "df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try using first comments only\n",
    "# df['comments_orig'] = df['comments_orig'].apply(lambda x: x.split('|__|')[0] if isinstance(x, str) else x)\n",
    "# df['comments'] = df['comments'].apply(lambda x: x.split('|__|')[0] if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Noise Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Noise Filter\n",
    "\n",
    "config = {\n",
    "    'MinLengths': {\n",
    "        'title': 10,\n",
    "        # 'comments': 10\n",
    "        'comments_orig': 10\n",
    "    },\n",
    "    'ExcludeImages': True,\n",
    "    'TextFilters': {\n",
    "        # 'subreddit': ['news', 'nottheonion'],\n",
    "    },\n",
    "}\n",
    "noise_filter = NoiseFilter(df, config)\n",
    "df_filtered = noise_filter.apply()\n",
    "df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Acceptance Indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_acceptance_indexes(title, comments, tag=None):\n",
    "    comment_list = comments.split(\"|__|\")\n",
    "    results = get_acceptance_indexes(title, comment_list)\n",
    "    return results\n",
    "\n",
    "def apply_indexers(df, ref_col, subject_col):\n",
    "    df_indexed = df.copy()\n",
    "    df_indexed = pd.concat([df_indexed, df_indexed.apply(lambda x: calculate_acceptance_indexes(x[ref_col], x[subject_col]), axis=1).apply(pd.Series)], axis=1)\n",
    "    \n",
    "    return df_indexed\n",
    "\n",
    "\n",
    "df_orig = apply_indexers(df_filtered, 'title', 'comments_orig')\n",
    "# df_clean = apply_indexers(df_filtered, 'clean_title', 'comments')\n",
    "df_orig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Datasets for next stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data out for the use in the ML Models\n",
    "df_orig.to_csv('data_with_indexers_applied_to_original_data.tsv', index=False, sep='\\t')\n",
    "# df_clean.to_csv('data_with_indexers_applied_to_clean_data.tsv', index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "The remainder is just evaluating the outputs of the Acceptance Indexers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfo_2 = df_orig[['title', 'comments_orig', LABEL_COL, 'TextBlobAcceptanceIndexer', 'NRCAcceptanceIndexer', 'VADERAcceptanceIndexer']]\n",
    "# dfc_2 = df_clean[['clean_title', 'comments', LABEL_COL, 'TextBlobAcceptanceIndexer', 'NRCAcceptanceIndexer', 'VADERAcceptanceIndexer']]\n",
    "dfo_2 = df_orig[[LABEL_COL, 'TextBlobAcceptanceIndexer', 'NRCAcceptanceIndexer', 'VADERAcceptanceIndexer']]\n",
    "# dfc_2 = df_clean[[LABEL_COL, 'TextBlobAcceptanceIndexer', 'NRCAcceptanceIndexer', 'VADERAcceptanceIndexer']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfo_2.plot(kind='line')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfo_2.plot(kind='line', subplots=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfo_2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "scatter_matrix(dfo_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfo_2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sources = {\n",
    "    'Orig': dfo_2,\n",
    "    # 'Clean': dfc_2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_threshold(df, col, threshold):\n",
    "    # print(f\"Checking {df[col]} > {threshold}\")\n",
    "    df[f\"{col}\"] = (df[col] > threshold).astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "indexer_cols = [\n",
    "    'TextBlobAcceptanceIndexer',\n",
    "    'NRCAcceptanceIndexer',\n",
    "    'VADERAcceptanceIndexer',\n",
    "    # 'CombinedAcceptanceIndexer'\n",
    "]\n",
    "\n",
    "for name, df in data_sources.items():\n",
    "    print(f\"Applying thresholds for {name}...\")\n",
    "    df3 = df.copy()\n",
    "    for col in indexer_cols:\n",
    "        # df3 = apply_threshold(df3, col, df3[col].mean())\n",
    "        df3 = apply_threshold(df3, col, (df3[col].min() + (df3[col].max() - df3[col].min()) / 2))\n",
    "\n",
    "\n",
    "    df3.plot(kind='line', subplots=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class Balancer to ensure equal number of True and False samples (just for threshold determination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_classes(df_in, ref_col):\n",
    "    df = df_in.copy()\n",
    "\n",
    "    # Count the number of 1s and 0s in the reference column\n",
    "    num_zeros = df[ref_col].value_counts()[0]\n",
    "    num_ones = df[ref_col].value_counts()[1]\n",
    "\n",
    "    # Determine the minimum number of rows to keep\n",
    "    min_rows = min(num_zeros, num_ones)\n",
    "\n",
    "    # Filter the results to have an equal number of 0s and 1s\n",
    "    if num_zeros > num_ones:\n",
    "        df = pd.concat([\n",
    "            df.loc[df[ref_col] == 0].sample(min_rows, random_state=42),\n",
    "            df.loc[df[ref_col] == 1]\n",
    "        ])\n",
    "    else:\n",
    "        df = pd.concat([\n",
    "            df.loc[df[ref_col] == 0],\n",
    "            df.loc[df[ref_col] == 1].sample(min_rows, random_state=42)\n",
    "        ])\n",
    "\n",
    "    print(f\"Equal split leaves {len(df[df[ref_col] == 1])} True values and {len(df[df[ref_col] == 0])} False values\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(df, subject, reference):\n",
    "    matches = df[subject] == df[reference]\n",
    "    match_rate = matches.mean() * 100\n",
    "\n",
    "    print(f\"Match Rate between {subject} and {reference}: {match_rate:.2f}%\")\n",
    "    \n",
    "    return match_rate\n",
    "\n",
    "\n",
    "for name, df in data_sources.items():\n",
    "    print(f\"Validating results for {name}...\")\n",
    "    df3 = df.copy()\n",
    "    df3 = balance_classes(df3, LABEL_COL)\n",
    "\n",
    "    for col in indexer_cols:\n",
    "        # print(col)\n",
    "        threshold = (df3[col].min() + (df3[col].max() - df3[col].min()) / 2)\n",
    "        print(f\"Applying threshold of {threshold} for {col}...\")\n",
    "        df3 = apply_threshold(df3, col, threshold)\n",
    "        validate(df3, col, LABEL_COL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Threshold Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def tune_thresholds(df, subject, reference):\n",
    "    thresholds = np.arange(df[subject].min(), df[subject].max(), 0.01)\n",
    "    best_threshold = None\n",
    "    best_accuracy = 0\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        df['predicted'] = (df[subject] >= threshold).astype(int)\n",
    "        # accuracy = (df['predicted'] == df[reference]).mean() * 100\n",
    "        # accuracy = df['predicted'].corr(df[reference])\n",
    "        # accuracy = abs(accuracy)\n",
    "        accuracy = accuracy_score(df['predicted'], df[reference])\n",
    "        if accuracy > best_accuracy:\n",
    "            best_threshold = threshold\n",
    "            best_accuracy = accuracy\n",
    "        # print(f\"Threshold: {threshold}, Accuracy: {accuracy}, Best Threshold: {best_threshold}, Best Accuracy: {best_accuracy}\")\n",
    "\n",
    "    return best_threshold, best_accuracy\n",
    "\n",
    "for name, df in data_sources.items():\n",
    "    print(f\"Validating results for {name}...\")\n",
    "    df4 = df.copy()\n",
    "    df4 = balance_classes(df4, LABEL_COL)\n",
    "    for col in indexer_cols:\n",
    "        th, acc = tune_thresholds(df4, col, LABEL_COL)\n",
    "        # print(f\"Match Rate between {col} and reference: {acc:.2f}% using a threshold of {th}\")\n",
    "        print(f\"Best Match Rate between {col} and reference: {acc:.2f} using a threshold of {th}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalisation of Acceptance Indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply normalisation to the Acceptance Indexes\n",
    "\n",
    "# Normalise column between 0 and 1\n",
    "def normalise_column(df, column):\n",
    "    min_value = df[column].min()\n",
    "    max_value = df[column].max()\n",
    "    df[f\"{column}_norm\"] = (df[column] - min_value) / (max_value - min_value)\n",
    "    return df\n",
    "\n",
    "scaled_sources = {}\n",
    "for name, df in data_sources.items():\n",
    "    print(f\"Normalising {name}...\")\n",
    "    df_scaled = df.copy()\n",
    "    for col in indexer_cols:\n",
    "        df_scaled = normalise_column(df_scaled, col)\n",
    "    \n",
    "    scaled_sources[name] = df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compare indexers using cleaned and uncleaned data\n",
    "df_scaled.plot(subplots=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for name, df in scaled_sources.items():\n",
    "    print(f\"Validating results for {name}...\")\n",
    "    df5 = df.copy()\n",
    "    df5 = balance_classes(df5, LABEL_COL)\n",
    "    for col in indexer_cols:\n",
    "        th, acc = tune_thresholds(df5, f\"{col}_norm\", LABEL_COL)\n",
    "        # print(f\"Match Rate between {col} and reference: {acc:.2f}% using a threshold of {th}\")\n",
    "        print(f\"Best Match Rate between {col} and reference: {acc:.2f} using a threshold of {th:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tus_etp_fake_news_detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

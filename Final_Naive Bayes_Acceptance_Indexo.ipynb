!pip install textblob
!pip install -U scikit-learn
!pip install nrclex
from google.colab import drive

import sys
import os

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.pipeline import Pipeline
from sklearn.naive_bayes import MultinomialNB
from sklearn.compose import ColumnTransformer
from textblob import TextBlob
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from nrclex import NRCLex
import nltk
nltk.download('punkt_tab')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('vader_lexicon')

drive.mount('/content/drive',force_remount=True)
# Add the 'libs' directory to the system path
drive_libs_path = '/content/drive/MyDrive/libs'
sys.path.append(drive_libs_path)

from usage_examples import get_acceptance_indexes, CombinedAcceptanceIndexer
from noise_filter import NoiseFilter

def mount_drive():
  drive.mount('/content/drive', force_remount=True)

def load_and_filter_data(file_path):
    df = pd.read_csv(file_path, low_memory=False, nrows=1000)

    # Apply noise filters (using example)
    filter_config = {
        'MinLengths': {
            'clean_title': 5,
            'comments': 5
        },
        'ExcludeImages': True
    }

    noise_filter = NoiseFilter(df, filter_config)
    filtered_df = noise_filter.apply()

    # Standard preprocessing
    filtered_df['clean_title'] = filtered_df['clean_title'].astype(str).str.lower()
    filtered_df['comments'] = filtered_df['comments'].astype(str)
    filtered_df['separated_comment'] = ''

    return filtered_df

def get_sentiment(text):
    # Assuming you have loaded the appropriate sentiment model
    blob = TextBlob(text)
    #analyzer = SentimentIntensityAnalyzer()  # Uncomment for Vader sentiment if installed and needed
    #scores = analyzer.polarity_scores(text)
    #polarity = scores['compound']  # For Vader sentiment
    #subjectivity = scores['neu']    # For Vader sentiment
    return blob.sentiment.polarity, blob.sentiment.subjectivity

def enhanced_preprocess_data(df):
    # Assuming process_comments and get_sentiment are defined and imported
    df['comments_polarity'], df['comments_subjectivity'] = zip(*df['comments'].apply(get_sentiment)) # Assuming process_comments is replaced with get_sentiment for comments
    df['clean_title_polarity'], df['clean_title_subjectivity'] = zip(*df['clean_title'].apply(get_sentiment))
    df['separated_comment'] = df['comments'].apply(lambda x: x.split('|__|'))
    # Calculate average comment polarity
    df['sentiment_comment_avg'] = df['separated_comment'].apply(lambda comments: 
                                                                np.mean([TextBlob(c).sentiment.polarity for c in comments if c]) 
                                                                if comments else np.nan)  
                                                                # Handle empty comments
    # Add acceptance index features
    df['acceptance_index'] = df.apply(
        lambda row: calculate_acceptance_index(row['clean_title'], row['comments']),
        axis=1
    )

    return df

def calculate_acceptance_index(title, comments):
    if pd.isna(comments) or comments == '':
        return 0

    comment_list = comments.split('|__|')
    indexer = CombinedAcceptanceIndexer(title, comment_list)
    return indexer.calculate_acceptance_index()


def prepare_features_and_labels(df):
    X = df[['clean_title', 'score', 'separated_comment', 'subreddit',
           'num_comments', 'upvote_ratio', 'comments_polarity',
           'comments_subjectivity', 'clean_title_polarity',
           'clean_title_subjectivity', 'acceptance_index','sentiment_comment_avg']]

    numerical_features = ['score', 'num_comments', 'upvote_ratio',
                         'comments_polarity', 'comments_subjectivity',
                         'clean_title_polarity', 'clean_title_subjectivity',
                         'acceptance_index','sentiment_comment_avg']

    # Impute missing numerical values with 0 (or the mean if preferred)
    for feature in numerical_features:
        if feature not in df.columns:
            df[feature] = 0  # Create a column filled with 0 if the feature is missing
        else:
            df[feature] = df[feature].fillna(0)  # Fill missing values with 0 in existing columns

    X[numerical_features] = X[numerical_features].fillna(0)


    y = df['2_way_label'].astype(str)
    label_encoder = LabelEncoder()
    y = label_encoder.fit_transform(y)

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    preprocessor = ColumnTransformer(
        transformers=[
            ('num', 'passthrough', numerical_features),
            ('text', TfidfVectorizer(max_features=5000), 'clean_title'),
            ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), ['subreddit']),  # Use sparse=False to avoid sparse matrices
        ])

    X_train = preprocessor.fit_transform(X_train)
    X_test = preprocessor.transform(X_test)

    # Ensure non-negative values for MultinomialNB
    X_train[X_train < 0] = 0
    X_test[X_test < 0] = 0

    return (X_train, X_test, y_train, y_test), label_encoder, preprocessor

def enhanced_predict_new_comments(model, label_encoder, preprocessor, comments):
    df_comments = pd.DataFrame({
        'clean_title': comments,
        'score': [0] * len(comments),
        'subreddit': ['unknown'] * len(comments),
        'num_comments': [0] * len(comments),
        'upvote_ratio': [0] * len(comments),
        'separated_comment': [''] * len(comments),
        'comments_polarity': [TextBlob(c).sentiment.polarity for c in comments],
        'comments_subjectivity': [TextBlob(c).sentiment.subjectivity for c in comments],
        'clean_title_polarity': [TextBlob(c).sentiment.polarity for c in comments],
        'clean_title_subjectivity': [TextBlob(c).sentiment.subjectivity for c in comments],
        'acceptance_index': [0] * len(comments),
        'sentiment_comment_avg': [0] * len(comments)
    })

    comments_transformed = preprocessor.transform(df_comments)
    predictions = model.predict(comments_transformed)

    results = []
    for comment, pred in zip(comments, predictions):
        blob = TextBlob(comment)
        results.append({
            'comment': comment,
            'prediction': label_encoder.inverse_transform([pred])[0],
            'polarity': blob.sentiment.polarity,
            'subjectivity': blob.sentiment.subjectivity
        })

    return results

def build_pipeline():
  model = Pipeline([
      ('classifier', MultinomialNB())  # Example: using Multinomial Naive Bayes
  ])
  return model

def evaluate_model(model, X_test, y_test, label_encoder):
  y_pred = model.predict(X_test)
  print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))
  print(f"Accuracy: {accuracy_score(y_test, y_pred):.2f}")


def main():
    # Mount drive and load data
    mount_drive()
    file_path = "/content/drive/My Drive/datasets/merged_cleaned_data_v31_news.csv"  # Update with your file path
    df = load_and_filter_data(file_path)

    # Enhanced preprocessing
    processed_df = enhanced_preprocess_data(df)

    # Prepare features and train model
    (X_train, X_test, y_train, y_test), label_encoder, preprocessor = prepare_features_and_labels(processed_df)
    model = build_pipeline()
    model.fit(X_train, y_train)

    # Evaluate
    evaluate_model(model, X_test, y_test, label_encoder)

    # Predict on new comments with sentiment analysis
    new_comments = [
        "This is outrageous! There's no way this can be true!",
        "I can't believe people are falling for this. It's just a hoax.",
        "Great news! This is exactly what we needed.",
        "Interesting perspective, I hadn't considered that before."
    ]

    predictions = enhanced_predict_new_comments(model, label_encoder, preprocessor, new_comments)

    print("\nEnhanced Predictions with Sentiment Analysis:")
    for result in predictions:
        print(f"\nComment: {result['comment']}")
        print(f"Prediction: {result['prediction']}")
        print(f"Polarity: {result['polarity']:.2f} (Negative to Positive)")
        print(f"Subjectivity: {result['subjectivity']:.2f} (Objective to Subjective)")

if __name__ == "__main__":
    main()
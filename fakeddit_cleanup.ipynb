{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas\n",
    "%pip install nltk\n",
    "%pip install scikit-learn\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load the datasets\n",
    "try:\n",
    "  df_train = pd.read_csv('/content/drive/MyDrive/TUS/Engineering_Project/data/source_files/all_train.tsv', sep='\\t', encoding='latin1', low_memory=True)\n",
    "  df_test = pd.read_csv('/content/drive/MyDrive/TUS/Engineering_Project/data/source_files/all_test_public.tsv', sep='\\t', encoding='latin1', low_memory=True)\n",
    "  df_validate = pd.read_csv('/content/drive/MyDrive/TUS/Engineering_Project/data/source_files/all_validate.tsv', sep='\\t', encoding='latin1', low_memory=True)\n",
    "\n",
    "except Exception as e:\n",
    "  print(f\"An error occurred during data loading: {e}\")\n",
    "\n",
    "try:\n",
    "  df_submissions_clean = pd.concat([df_train, df_test, df_validate], ignore_index=True)\n",
    "except Exception as e:\n",
    "  print(f\"An error occurred during concatenation: {e}\")\n",
    "\n",
    "# Save the cleaned DataFrame to a new CSV file\n",
    "try:\n",
    "    df_submissions_clean.to_csv('/content/drive/MyDrive/TUS/Engineering_Project/data/concatted_all_data.csv', index=False)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred saving to csv: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import glob\n",
    "import csv\n",
    "\n",
    "# Configuration class for cleaning parameters\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.chunk_size = 10000\n",
    "        self.max_text_length = 500\n",
    "        self.comment_invalid_phrases = [\n",
    "            'post comment without original photoshop',\n",
    "            'please post comment photoshop reply',\n",
    "            'unfortunately, your submission has been removed from',\n",
    "            'google image search automated response',\n",
    "            'is a curated space.**',\n",
    "            'In order not to get your comment removed',\n",
    "            'm a bot so if I was wrong'\n",
    "        ]\n",
    "        # self.stop_words = set(stopwords.words('english'))\n",
    "        self.placeholder = \"placeholderspecialdelim\"\n",
    "\n",
    "# Cleaner class to encapsulate data cleaning\n",
    "class Cleaner:\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        # self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        if isinstance(text, float):  # Handle NaN\n",
    "            print(f\"[DEBUG] Encountered NaN or invalid text: {text}\")\n",
    "            return ''\n",
    "\n",
    "        # Temporarily replace special delimiter\n",
    "        text = text.replace('|__|', self.config.placeholder)\n",
    "        print(f\"[DEBUG] After replacing special delimiter: {text}\")\n",
    "\n",
    "        # Remove unwanted characters\n",
    "        text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "        text = re.sub(r'<.*?>', '', text)  # Remove HTML tags\n",
    "        text = re.sub(r'[^a-zA-Z\\s' + self.config.placeholder + ']', '', text)  # Remove special characters\n",
    "        print(f\"[DEBUG] After cleaning unwanted characters: {text}\")\n",
    "\n",
    "        text = self.use_space_instead_of_chars(text)\n",
    "\n",
    "        # Convert text to lowercase\n",
    "        text = text.lower()\n",
    "        print(f\"[DEBUG] Converted text to lowercase: {text}\")\n",
    "\n",
    "        # Truncate text if it exceeds the maximum length\n",
    "        if len(text) > self.config.max_text_length:\n",
    "            print(f\"[DEBUG] Truncating text to max length of {self.config.max_text_length}\")\n",
    "            text = text[:self.config.max_text_length] + '...'\n",
    "            #text = text[:self.config.max_text_length] + '...' + '|__|'\n",
    "\n",
    "        return text\n",
    "\n",
    "    def filter_csv_compliant(self, text):\n",
    "        \"\"\"\n",
    "        Filters and converts a given text string to be compliant with CSV format.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input text string to be filtered.\n",
    "\n",
    "        Returns:\n",
    "            str: The CSV-compliant text string.\n",
    "        \"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            raise ValueError(\"Input must be a string\")\n",
    "\n",
    "        text = self.use_space_instead_of_chars(text)\n",
    "        # Escape special characters for CSV\n",
    "        escaped_text = csv.StringIO()\n",
    "        writer = csv.writer(escaped_text, quoting=csv.QUOTE_MINIMAL)\n",
    "        writer.writerow([text])\n",
    "\n",
    "        # Truncate text if it exceeds the maximum length\n",
    "        if len(text) > self.config.max_text_length:\n",
    "            print(f\"[DEBUG] Truncating text to max length of {self.config.max_text_length}\")\n",
    "            text = text[:self.config.max_text_length] + '...'\n",
    "\n",
    "        # Return filtered text without surrounding quotes\n",
    "        return escaped_text.getvalue().strip()\n",
    "\n",
    "    def use_space_instead_of_chars(self, text):\n",
    "        # Replace tabs and commas\n",
    "        text = text.replace('\\t', ' ')  # Replace tabs with space\n",
    "        text = text.replace(',', ' ')  # Replace comma with space\n",
    "        text = text.replace('\\n', ' ')  # Replace comma with space\n",
    "        text = text.replace('\\r', ' ')  # Replace comma with space\n",
    "        return text  # Return the modified text\n",
    "\n",
    "    def filter_comments(self, df_comments):\n",
    "        comments_dict = {}\n",
    "        comments_orig_dict = {}\n",
    "\n",
    "        print(f\"[DEBUG] Total comments before filtering: {len(df_comments)}\")\n",
    "\n",
    "        for i, row in df_comments.iterrows():\n",
    "            parent_id = row.get('parent_id', '')\n",
    "            if isinstance(parent_id, str):\n",
    "                parent_id = parent_id.split('_', 1)[-1]  # Remove prefix like 't1_'\n",
    "                print(f\"[DEBUG] Processed parent_id: {parent_id}\")\n",
    "\n",
    "            comment_body = row.get('body', '') if pd.notna(row.get('body', '')) else ''\n",
    "            print(f\"[DEBUG] Original comment body: {comment_body}\")\n",
    "\n",
    "            # Skip invalid comments\n",
    "            if (comment_body in ['NaN', 'deleted', 'removed', '', '[deleted]', '[removed]']) or \\\n",
    "               any(phrase in comment_body.lower() for phrase in self.config.comment_invalid_phrases):\n",
    "                print(f\"[DEBUG] Skipping invalid comment: {comment_body}\")\n",
    "                continue\n",
    "            elif parent_id != '' and comment_body != '':\n",
    "                # Clean comment body\n",
    "                comment_body = self.filter_csv_compliant(comment_body)\n",
    "                # print(f\"[DEBUG] CSV compliant comment body: {comment_body}\")\n",
    "                # Build comment dictionaries\n",
    "                if parent_id in comments_orig_dict:\n",
    "                    # comments_dict[parent_id].append(cleaned_comment)\n",
    "                    comments_orig_dict[parent_id].append(comment_body)\n",
    "                    print(f\"[DEBUG] Appended comment to parent_id {parent_id}\")\n",
    "                else:\n",
    "                    # comments_dict[parent_id] = [cleaned_comment]\n",
    "                    comments_orig_dict[parent_id] = [comment_body]\n",
    "                    print(f\"[DEBUG] Created new entry for parent_id {parent_id}\")\n",
    "\n",
    "        print(f\"[DEBUG] Filtered {len(comments_orig_dict)} unique parent_ids with valid comments.\")\n",
    "        return comments_dict, comments_orig_dict\n",
    "\n",
    "    def clean_dataframe(self, df):\n",
    "        print(f\"[DEBUG] Starting dataframe cleaning. Initial columns: {df.columns}\")\n",
    "\n",
    "        # Remove unnamed columns\n",
    "        unnamed_cols = [col for col in df.columns if col.startswith('Unnamed')]\n",
    "        df.drop(columns=unnamed_cols, inplace=True)\n",
    "        print(f\"[DEBUG] Removed unnamed columns. Remaining columns: {df.columns}\")\n",
    "\n",
    "        # Filter rows based on 'hasImage' and 'subreddit'\n",
    "        initial_row_count = len(df)\n",
    "        df = df[~df['hasImage'].astype(str).str.lower().eq('true')]\n",
    "        print(f\"[DEBUG] Rows after removing hasImage=True: {len(df)} (removed {initial_row_count - len(df)})\")\n",
    "        df = df[~df['subreddit'].str.lower().str.contains('photo')]\n",
    "        print(f\"[DEBUG] Rows after removing subreddits containing 'photo': {len(df)}\")\n",
    "        # df = df[df['subreddit'].str.lower().str.contains('news')]\n",
    "        # print(f\"[DEBUG] Rows after filtering subreddits containing 'news': {len(df)}\")\n",
    "\n",
    "        # Drop unnecessary columns\n",
    "        drop_columns = ['created_utc', 'domain', 'image_url']\n",
    "        df.drop(columns=drop_columns, inplace=True, errors='ignore')\n",
    "        print(f\"[DEBUG] Dropped columns: {drop_columns}. Remaining columns: {df.columns}\")\n",
    "\n",
    "        return df\n",
    "\n",
    "# Main function to process data in chunks\n",
    "def process_data_in_chunks(config: Config, cleaner: Cleaner, submissions_path, comments_path, output_path):\n",
    "    try:\n",
    "\n",
    "        # Toggle for using static test data\n",
    "        USE_STATIC_DATA = False\n",
    "\n",
    "        # Static test data\n",
    "        static_comments_data = {\n",
    "            'id': [3665229, 8035415, 8261647, 8261648, 8261649, 8261650, 8261651, 8261652, 8261653, 8261654, 8261655, 8261656, 8261657, 8261658, 8261659, 8261660, 8261661, 8261662, 8261663, 8261664, 8261665, 8261666, 8261667, 8261668, 8261669, 8261670, 8261671, 8261672, 8261673, 8261674, 8261675, 8261676, 8261677, 8261678, 8261679, 8261680, 8261681, 8261682, 8261683, 8261684, 8241541, 8241542, 8241543, 8241544, 8241545, 8231680, 8231681, 8231682],\n",
    "            'author': ['kutuup1989', 'cpzjcsn', 'RabbiVolesSolo', 'now_stop_that', '', 'Gwunt', 'Kuritos', 'msbutah', 'HomerJunior', '', '', 'IratePieRater', 'MakesPensDance', 'showmeyourlove', 'Tetragrade', 'corvustock', 'CrispyPudding', 'lifeisjest', 'somestupidname1', 'GoghGirl', 'GoghGirl', 'paulrulez742', 'paulrulez742', 'paulrulez742', 'paulrulez742', 'paulrulez742', 'paulrulez742', 'paulrulez742', 'paulrulez742', 'paulrulez742', 'paulrulez742', 'paulrulez742', 'paulrulez742', 'paulrulez742', 'paulrulez742', 'paulrulez742', 'paulrulez742', 'paulrulez742', 'IratePieRater', 'paulrulez742', '', 'Mobbasta', '', 'DoofusMagnus', 'ELF4000', 'stimpakish', 'phd_dude', ''],\n",
    "            'body': [\"I may have spoken too soon in my previous comment.\\n\\nRelevant:\\n\\nKaren Raines Keller\\n\\nResponses from Blakely Elementary School to the Lego article...\", \"?\", \";)\", \";)\", \";)\", \";)\", \";)\", \";)\", \"( ͡~ ͜ʖ ͡°)\", \"[deleted]\", \"[deleted]\", \";)\", \";)\", \";)\", \":^)\", \";)\", \"+‿-\", \"Some really weird shit pops up in r/all ...\", \";)\", \"^(^-^) ^\", \"(;\", \";)\", \";)\", \";)\", \";)\", \";)\", \";)\", \";)\", \";)\", \";)\", \";)\", \";)\", \";)\", \";)\", \";)\", \";)\", \";)\", \";)\", \"^(^—^) ^\", \"(;\", \"\\\"One of us. One of us.\\\"\", \"._.\", \"•____•\", \".▂.\", \".-.\", \":[]\", \":[ ]\", \"[deleted]\"],\n",
    "            'isTopLevel': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, True, True, True, True, False, False],\n",
    "            'parent_id': ['t3_3tjd87', 't3_3148se', 't3_3iqltn', 't3_3iqltn', 't3_3iqltn', 't3_3iqltn', 't3_3iqltn', 't3_3iqltn', 't3_3iqltn', 't3_3iqltn', 't3_3iqltn', 't3_3iqltn', 't3_3iqltn', 't3_3iqltn', 't3_3iqltn', 't3_3iqltn', 't3_3iqltn', 't3_3iqltn', 't3_3iqltn', 't3_3iqltn', 't3_3iqltn', 't1_cuiqotf', 't1_cuitm7b', 't1_cuiu4f7', 't1_cuj34lk', 't1_cuj349r', 't1_cuj64dt', 't1_cuj7uzl', 't1_cujjn82', 't1_cujl3rx', 't1_cujarhm', 't1_cujcwje', 't1_cujebjb', 't1_cuivqrs', 't1_cujmqmf', 't1_cujoke2', 't1_cuj7h2o', 't1_cujgz0t', 't1_cujt1t1', 't1_cujj9h0', 't3_53pohv', 't3_53ov0q', 't3_53ov0q', 't3_53ov0q', 't3_53ov0q', 't3_5rpn5l', 't1_dd954mt', 't1_dd962ss'],\n",
    "            'submission_id': ['3tjd87', '3148se', '3iqltn', '3iqltn', '3iqltn', '3iqltn', '3iqltn', '3iqltn', '3iqltn', '3iqltn', '3iqltn', '3iqltn', '3iqltn', '3iqltn', '3iqltn', '3iqltn', '3iqltn', '3iqltn', '3iqltn', '3iqltn', '3iqltn', '3iqltn', '3iqltn', '3iqltn', '3iqltn', '3iqltn', '3iqltn', '3iqltn', '3iqltn', '3iqltn', '3iqltn', '3iqltn', '3iqltn', '3iqltn', '3iqltn', '3iqltn', '3iqltn', '3iqltn', '3iqltn', '3iqltn', '53pohv', '53ov0q', '53ov0q', '53ov0q', '53ov0q', '5rpn5l', '5rpn5l', '5rpn5l'],\n",
    "            'ups': [7.0, 3.0, 16.0, 11.0, 9.0, 7.0, 6.0, 7.0, 8.0, 3.0, 3.0, 4.0, 4.0, 6.0, 5.0, 2.0, 2.0, 4.0, 2.0, 1.0, 1.0, 6.0, 7.0, 9.0, 7.0, 5.0, 1.0, 5.0, 1.0, 2.0, 2.0, 3.0, 1.0, 4.0, 2.0, 2.0, 4.0, 1.0, 2.0, 2.0, 1.0, 15.0, 6.0, 5.0, 1.0, 1.0, 1.0, 1.0]\n",
    "        }\n",
    "\n",
    "        static_submissions_data = {\n",
    "            'Unnamed: 0': [2402, 10646, 149720, 744801, 167700, 456042],\n",
    "            'Unnamed: 0.1': [2402, 10646, 149720, 744801, 167700, 456042],\n",
    "            'Unnamed: 0.1.1': [None, None, None, None, None, None],\n",
    "            'author': ['scapler', 'rpilek', 'paulrulez742', 'tinywinner', 'Wizard-ette', 'patho5'],\n",
    "            'clean_title': ['blakely teacher restricts legoplay to her girl students in the pursuit of gender equity', 'australian wwi poster', 'sure sit on me and spin', 'this horde of ikea drawer stoppers', 'oo', ':[]'],\n",
    "            'created_utc': [1448000720.0, 1427925770.0, 1440776896.0, 1474406090.0, 1474396501.0, 1486071264.0],\n",
    "            'domain': ['bainbridgereview.com', 'i.imgur.com', 'imgur.com', 'i.reddituploads.com', 'imgur.com', 'i.reddituploads.com'],\n",
    "            'hasImage': [False, True, True, True, True, True],\n",
    "            'id': ['3tjd87', '3148se', '3iqltn', '53pohv', '53ov0q', '5rpn5l'],\n",
    "            'image_url': [\n",
    "                '',\n",
    "                'https://external-preview.redd.it/4xOStvKJZnwjGoP5gqEnms7etyLq6pRFp7h0JeWKcsU.jpg?width=320&crop=smart&auto=webp&s=cfde9ddf38c4916dacfb5c424d4de3507799c32e',\n",
    "                'https://external-preview.redd.it/Lc7LWsIEOudLxy-n-TENIY2BshKJBpQbg0Z7NfcKuy4.jpg?width=320&crop=smart&auto=webp&s=9249dd6b86aeb2aafd6f2de07582d3874026d7c5',\n",
    "                'https://external-preview.redd.it/Z6P_PMpruvYHpjE7ZA72IJxva-MYzxQRSojcQ7MY5xQ.jpg?width=320&crop=smart&auto=webp&s=4f4d318cc07d58ccee4424f225c0f700e57ba917',\n",
    "                'https://external-preview.redd.it/_nMpG6S5mOrZH53Z190jRm5XACqubYFblL3rw_0v0yc.jpg?width=320&crop=smart&auto=webp&s=8ae0082fcafd2397225b7c9ee3fc8a6e7f987368',\n",
    "                'https://external-preview.redd.it/GWZg6zeKP0Opjzb1xSixCMyadWkjs2xVOV-B6izCfvQ.jpg?width=320&crop=smart&auto=webp&s=0a424b912b60aafeaeda7ab88d0001d8ce1f3fd9'\n",
    "            ],\n",
    "            'linked_submission_id': [None, None, None, None, None, None],\n",
    "            'num_comments': [604.0, 2.0, 39.0, 1.0, 4.0, 3.0],\n",
    "            'score': [1327, 36, 720, 15, 162, 11],\n",
    "            'subreddit': ['nottheonion', 'propagandaposters', 'pareidolia', 'pareidolia', 'pareidolia', 'pareidolia'],\n",
    "            'title': ['Blakely teacher restricts Lego-play to her girl students in the pursuit of gender equity', 'Australian WWI Poster. 1914-1918.', 'Sure, sit on me and spin', 'This horde of IKEA drawer stoppers.', 'o_o', ':[]'],\n",
    "            'upvote_ratio': [0.89, 0.95, 0.96, 0.74, 0.97, 0.92],\n",
    "            '2_way_label': [1, 0, 0, 0, 0, 0],\n",
    "            '3_way_label': [0, 1, 2, 2, 2, 2],\n",
    "            '6_way_label': [0, 5, 2, 2, 2, 2]\n",
    "        }\n",
    "\n",
    "\n",
    "        # Load the datasets\n",
    "        if USE_STATIC_DATA:\n",
    "            # df_submissions_clean = pd.DataFrame(static_submissions_data)\n",
    "            df_submissions_clean = pd.DataFrame(static_submissions_data)\n",
    "            df_comments = pd.DataFrame(static_comments_data)\n",
    "            chunk_iterator = [df_submissions_clean]  # Wrap the DataFrame in a list for iteration\n",
    "        else:\n",
    "            # Load comments\n",
    "            print(\"[DEBUG] Loading comments dataset...\")\n",
    "            df_comments = pd.read_csv(comments_path, sep='\\t', encoding='utf-8', on_bad_lines='warn', low_memory=True)\n",
    "            # Load submissions in chunks\n",
    "            print(\"[DEBUG] Loading submissions dataset in chunks...\")\n",
    "            chunk_iterator = pd.read_csv(submissions_path, encoding='latin1', low_memory=True, on_bad_lines='skip', chunksize=config.chunk_size)\n",
    "\n",
    "        comments_dict, comments_orig_dict = cleaner.filter_comments(df_comments)\n",
    "\n",
    "        for chunk_count, chunk in enumerate(chunk_iterator):\n",
    "            print(f\"[DEBUG] Processing chunk {chunk_count + 1} with {len(chunk)} rows...\")\n",
    "            chunk = cleaner.clean_dataframe(chunk)\n",
    "\n",
    "            # Map comments to submissions\n",
    "            print(\"[DEBUG] Mapping comments to submissions...\")\n",
    "            chunk['comments_orig'] = chunk.apply(\n",
    "                # lambda row: '|__|'.join(set(comments_orig_dict.get(row.get('linked_submission_id', ''), []) +\n",
    "                #                            comments_orig_dict.get(row.get('id', ''), []))),\n",
    "                lambda row: '|__|'.join(set(comments_orig_dict.get(row.get('id', ''), []))),\n",
    "                axis=1\n",
    "            )\n",
    "            print(\"[DEBUG] Finished mapping comments for chunk.\")\n",
    "\n",
    "            # Remove rows without comments\n",
    "            initial_chunk_row_count = len(chunk)\n",
    "            chunk = chunk[chunk['comments_orig'] != '']\n",
    "            print(f\"[DEBUG] Rows after filtering out empty comments: {len(chunk)} (removed {initial_chunk_row_count - len(chunk)})\")\n",
    "\n",
    "            # Save cleaned chunk\n",
    "            chunk.to_csv(f\"{output_path}/cleaned_chunk_{chunk_count + 1}_no-photo.csv\", index=False)\n",
    "            print(f\"[DEBUG] Chunk {chunk_count + 1} saved with {len(chunk)} rows.\")\n",
    "\n",
    "        # Combine all cleaned chunks into a single file\n",
    "        all_files = glob.glob(f\"{output_path}/cleaned_chunk_*_no-photo.csv\")\n",
    "        print(f\"[DEBUG] Combining {len(all_files)} cleaned chunk files...\")\n",
    "        combined_df = pd.concat([pd.read_csv(f) for f in all_files], ignore_index=True)\n",
    "        combined_df.to_csv(f\"{output_path}/merged_cleaned_data_v33_no_photo_v6.csv\", index=False)\n",
    "        print(\"[DEBUG] All chunks merged and saved to merged_cleaned_data.csv.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] An error occurred: {e}\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    config = Config()\n",
    "    cleaner = Cleaner(config)\n",
    "\n",
    "    # Paths\n",
    "    submissions_path = '/content/drive/MyDrive/TUS/Engineering_Project/data/concatted_all_data.csv'\n",
    "    comments_path = '/content/drive/MyDrive/TUS/Engineering_Project/data/source_files/all_comments.tsv'\n",
    "    output_path = '/content/drive/MyDrive/TUS/Engineering_Project/data/cleaned_chunks'\n",
    "\n",
    "    # Process data\n",
    "    process_data_in_chunks(config, cleaner, submissions_path, comments_path, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Export all FAKE subreddit titles\n",
    "# import pandas as pd\n",
    "# import logging\n",
    "\n",
    "# # Set up logging\n",
    "# logging.basicConfig(filename='query_results.log', level=logging.INFO, \n",
    "#                     format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# class DataInMemory:\n",
    "#     def __init__(self, submissions_path):\n",
    "#         self.submissions_path = submissions_path\n",
    "#         self.df_submissions = None\n",
    "\n",
    "#     def load_data(self):\n",
    "#         try:\n",
    "#             # Load the submissions dataset into memory\n",
    "#             print(\"[DEBUG] Loading submissions dataset...\")\n",
    "#             self.df_submissions = pd.read_csv(self.submissions_path, encoding='latin1', low_memory=True)\n",
    "#             print(f\"[DEBUG] Submissions dataset loaded with {len(self.df_submissions)} rows.\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"[ERROR] An error occurred while loading submissions data: {e}\")\n",
    "\n",
    "#     def filter_and_export_submissions(self, output_file):\n",
    "#         try:\n",
    "#             print(\"[DEBUG] Filtering submissions with upvote_ratio > 0.90...\")\n",
    "\n",
    "#             # Filter rows where 'upvote_ratio' > 0.90\n",
    "#             filtered_df = self.df_submissions[self.df_submissions['upvote_ratio'] > 0.90]\n",
    "#             print(f\"[DEBUG] Filtered {len(filtered_df)} submissions with upvote_ratio > 0.90.\")\n",
    "\n",
    "#             # Export to CSV\n",
    "#             filtered_df.to_csv(output_file, index=False)\n",
    "#             print(f\"[DEBUG] Filtered submissions exported to {output_file}.\")\n",
    "            \n",
    "#             # Log results\n",
    "#             logging.info(f\"Filtered {len(filtered_df)} submissions with upvote_ratio > 0.90 and exported to {output_file}.\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"[ERROR] An error occurred during filtering or exporting: {e}\")\n",
    "#             logging.error(f\"An error occurred during filtering or exporting: {e}\")\n",
    "\n",
    "# # Main execution\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Define paths\n",
    "#     submissions_path = '/content/drive/MyDrive/TUS/Engineering_Project/data/concatted_all_data.csv'\n",
    "#     output_file = '/content/drive/MyDrive/TUS/Engineering_Project/data/filtered_submissions_fake_ones.csv'\n",
    "\n",
    "#     # Initialize and load datasets into memory\n",
    "#     data_manager = DataInMemory(submissions_path)\n",
    "#     data_manager.load_data()\n",
    "\n",
    "#     # Filter and export submissions with upvote_ratio > 0.90\n",
    "#     data_manager.filter_and_export_submissions(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # No PHOTO submissions - original comments - just cpmpatile with EXCEL - no stopwors or nltk cleaning\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import glob\n",
    "import csv\n",
    "\n",
    "# Configuration class for cleaning parameters\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.chunk_size = 10000\n",
    "        self.max_text_length = 500\n",
    "        self.comment_invalid_phrases = [\n",
    "            'post comment without original photoshop',\n",
    "            'please post comment photoshop reply',\n",
    "            'unfortunately, your submission has been removed from',\n",
    "            'google image search automated response',\n",
    "            'is a curated space.**',\n",
    "            'In order not to get your comment removed',\n",
    "            'm a bot so if I was wrong'\n",
    "        ]\n",
    "        # self.stop_words = set(stopwords.words('english'))\n",
    "        self.placeholder = \"placeholderspecialdelim\"\n",
    "\n",
    "# Cleaner class to encapsulate data cleaning\n",
    "class Cleaner:\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        # self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        if isinstance(text, float):  # Handle NaN\n",
    "            print(f\"[DEBUG] Encountered NaN or invalid text: {text}\")\n",
    "            return ''\n",
    "\n",
    "        # Temporarily replace special delimiter\n",
    "        text = text.replace('|__|', self.config.placeholder)\n",
    "        print(f\"[DEBUG] After replacing special delimiter: {text}\")\n",
    "\n",
    "        # Remove unwanted characters\n",
    "        text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "        text = re.sub(r'<.*?>', '', text)  # Remove HTML tags\n",
    "        text = re.sub(r'[^a-zA-Z\\s' + self.config.placeholder + ']', '', text)  # Remove special characters\n",
    "        print(f\"[DEBUG] After cleaning unwanted characters: {text}\")\n",
    "\n",
    "        # Convert text to lowercase\n",
    "        text = text.lower()\n",
    "        print(f\"[DEBUG] Converted text to lowercase: {text}\")\n",
    "\n",
    "        # Truncate text if it exceeds the maximum length\n",
    "        if len(text) > self.config.max_text_length:\n",
    "            print(f\"[DEBUG] Truncating text to max length of {self.config.max_text_length}\")\n",
    "            text = text[:self.config.max_text_length] + '...' + '|__|'\n",
    "\n",
    "        return text\n",
    "\n",
    "    def filter_csv_compliant(self, text):\n",
    "        \"\"\"\n",
    "        Filters and converts a given text string to be compliant with CSV format.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input text string to be filtered.\n",
    "\n",
    "        Returns:\n",
    "            str: The CSV-compliant text string.\n",
    "        \"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            raise ValueError(\"Input must be a string\")\n",
    "\n",
    "        # Escape special characters for CSV\n",
    "        escaped_text = csv.StringIO()\n",
    "        writer = csv.writer(escaped_text, quoting=csv.QUOTE_MINIMAL)\n",
    "        writer.writerow([text])\n",
    "        \n",
    "        # Return filtered text without surrounding quotes\n",
    "        return escaped_text.getvalue().strip()\n",
    "\n",
    "    def filter_comments(self, df_comments):\n",
    "        comments_dict = {}\n",
    "        comments_orig_dict = {}\n",
    "\n",
    "        print(f\"[DEBUG] Total comments before filtering: {len(df_comments)}\")\n",
    "\n",
    "        for i, row in df_comments.iterrows():\n",
    "            parent_id = row.get('parent_id', '')\n",
    "            if isinstance(parent_id, str):\n",
    "                parent_id = parent_id.split('_', 1)[-1]  # Remove prefix like 't1_'\n",
    "                print(f\"[DEBUG] Processed parent_id: {parent_id}\")\n",
    "\n",
    "            comment_body = row.get('body', '') if pd.notna(row.get('body', '')) else ''\n",
    "            print(f\"[DEBUG] Original comment body: {comment_body}\")\n",
    "\n",
    "            # Skip invalid comments\n",
    "            if (comment_body in ['NaN', 'deleted', 'removed', '', '[deleted]', '[removed]']) or \\\n",
    "               any(phrase in comment_body.lower() for phrase in self.config.comment_invalid_phrases):\n",
    "                print(f\"[DEBUG] Skipping invalid comment: {comment_body}\")\n",
    "                continue\n",
    "            elif parent_id != '' and comment_body != '':\n",
    "                # Clean comment body\n",
    "                comment_body = self.filter_csv_compliant(comment_body)\n",
    "                # print(f\"[DEBUG] CSV compliant comment body: {comment_body}\")\n",
    "                # Build comment dictionaries\n",
    "                if parent_id in comments_orig_dict:\n",
    "                    # comments_dict[parent_id].append(cleaned_comment)\n",
    "                    comments_orig_dict[parent_id].append(comment_body)\n",
    "                    print(f\"[DEBUG] Appended comment to parent_id {parent_id}\")\n",
    "                else:\n",
    "                    # comments_dict[parent_id] = [cleaned_comment]\n",
    "                    comments_orig_dict[parent_id] = [comment_body]\n",
    "                    print(f\"[DEBUG] Created new entry for parent_id {parent_id}\")\n",
    "\n",
    "        print(f\"[DEBUG] Filtered {len(comments_orig_dict)} unique parent_ids with valid comments.\")\n",
    "        return comments_dict, comments_orig_dict\n",
    "\n",
    "    def clean_dataframe(self, df):\n",
    "        print(f\"[DEBUG] Starting dataframe cleaning. Initial columns: {df.columns}\")\n",
    "\n",
    "        # Remove unnamed columns\n",
    "        unnamed_cols = [col for col in df.columns if col.startswith('Unnamed')]\n",
    "        df.drop(columns=unnamed_cols, inplace=True)\n",
    "        print(f\"[DEBUG] Removed unnamed columns. Remaining columns: {df.columns}\")\n",
    "\n",
    "        # Filter rows based on 'hasImage' and 'subreddit'\n",
    "        initial_row_count = len(df)\n",
    "        df = df[~df['hasImage'].astype(str).str.lower().eq('true')]\n",
    "        print(f\"[DEBUG] Rows after removing hasImage=True: {len(df)} (removed {initial_row_count - len(df)})\")\n",
    "        df = df[~df['subreddit'].str.lower().str.contains('photo')]\n",
    "        print(f\"[DEBUG] Rows after removing subreddits containing 'photo': {len(df)}\")\n",
    "        # df = df[df['subreddit'].str.lower().str.contains('news')]\n",
    "        # print(f\"[DEBUG] Rows after filtering subreddits containing 'news': {len(df)}\")\n",
    "\n",
    "        # Drop unnecessary columns\n",
    "        drop_columns = ['created_utc', 'domain', 'image_url']\n",
    "        df.drop(columns=drop_columns, inplace=True, errors='ignore')\n",
    "        print(f\"[DEBUG] Dropped columns: {drop_columns}. Remaining columns: {df.columns}\")\n",
    "\n",
    "        return df\n",
    "\n",
    "# Main function to process data in chunks\n",
    "def process_data_in_chunks(config: Config, cleaner: Cleaner, submissions_path, comments_path, output_path):\n",
    "    try:\n",
    "        # Load comments\n",
    "        print(\"[DEBUG] Loading comments dataset...\")\n",
    "        df_comments = pd.read_csv(comments_path, sep='\\t', encoding='utf-8', on_bad_lines='warn', low_memory=True)\n",
    "        print(f\"[DEBUG] Loaded {len(df_comments)} rows from comments dataset.\")\n",
    "\n",
    "        comments_dict, comments_orig_dict = cleaner.filter_comments(df_comments)\n",
    "\n",
    "        # Load submissions in chunks\n",
    "        print(\"[DEBUG] Loading submissions dataset in chunks...\")\n",
    "        chunk_iterator = pd.read_csv(submissions_path, encoding='latin1', low_memory=True, on_bad_lines='skip', chunksize=config.chunk_size)\n",
    "\n",
    "        for chunk_count, chunk in enumerate(chunk_iterator):\n",
    "            print(f\"[DEBUG] Processing chunk {chunk_count + 1} with {len(chunk)} rows...\")\n",
    "            chunk = cleaner.clean_dataframe(chunk)\n",
    "\n",
    "            # Map comments to submissions\n",
    "            print(\"[DEBUG] Mapping comments to submissions...\")\n",
    "            chunk['comments_orig'] = chunk.apply(\n",
    "                # lambda row: '|__|'.join(set(comments_orig_dict.get(row.get('linked_submission_id', ''), []) +\n",
    "                #                            comments_orig_dict.get(row.get('id', ''), []))),\n",
    "                lambda row: '|__|'.join(set(comments_orig_dict.get(row.get('id', ''), []))),\n",
    "                axis=1\n",
    "            )\n",
    "            print(\"[DEBUG] Finished mapping comments for chunk.\")\n",
    "\n",
    "            # Remove rows without comments\n",
    "            initial_chunk_row_count = len(chunk)\n",
    "            chunk = chunk[chunk['comments_orig'] != '']\n",
    "            print(f\"[DEBUG] Rows after filtering out empty comments: {len(chunk)} (removed {initial_chunk_row_count - len(chunk)})\")\n",
    "\n",
    "            # Save cleaned chunk\n",
    "            chunk.to_csv(f\"{output_path}/cleaned_chunk_{chunk_count + 1}_no-photo.csv\", index=False)\n",
    "            print(f\"[DEBUG] Chunk {chunk_count + 1} saved with {len(chunk)} rows.\")\n",
    "\n",
    "        # Combine all cleaned chunks into a single file\n",
    "        all_files = glob.glob(f\"{output_path}/cleaned_chunk_*_no-photo.csv\")\n",
    "        print(f\"[DEBUG] Combining {len(all_files)} cleaned chunk files...\")\n",
    "        combined_df = pd.concat([pd.read_csv(f) for f in all_files], ignore_index=True)\n",
    "        combined_df.to_csv(f\"{output_path}/merged_cleaned_data_v33_no_photo.csv\", index=False)\n",
    "        print(\"[DEBUG] All chunks merged and saved to merged_cleaned_data.csv.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] An error occurred: {e}\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    config = Config()\n",
    "    cleaner = Cleaner(config)\n",
    "\n",
    "    # Paths\n",
    "    submissions_path = '/content/drive/MyDrive/TUS/Engineering_Project/data/concatted_all_data.csv'\n",
    "    comments_path = '/content/drive/MyDrive/TUS/Engineering_Project/data/source_files/all_comments.tsv'\n",
    "    output_path = '/content/drive/MyDrive/TUS/Engineering_Project/data/cleaned_chunks'\n",
    "\n",
    "    # Process data\n",
    "    process_data_in_chunks(config, cleaner, submissions_path, comments_path, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get data from 3 files by id(for comments mapped to submission_id or parent_id)\n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "# import logging\n",
    "\n",
    "# # Set up logging\n",
    "# logging.basicConfig(filename='query_results.log', level=logging.INFO, \n",
    "#                     format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# class DataInMemory:\n",
    "#     def __init__(self, submissions_path, comments_path, filtered_submissions_path):\n",
    "#         self.submissions_path = submissions_path\n",
    "#         self.comments_path = comments_path\n",
    "#         self.filtered_submissions_path = filtered_submissions_path\n",
    "#         self.df_submissions = None\n",
    "#         self.df_comments = None\n",
    "#         self.df_filtered_submissions = None\n",
    "\n",
    "#     def load_data(self):\n",
    "#         try:\n",
    "#             # Load datasets into memory\n",
    "#             print(\"[DEBUG] Loading submissions dataset...\")\n",
    "#             self.df_submissions = pd.read_csv(self.submissions_path, encoding='latin1', low_memory=True)\n",
    "#             print(f\"[DEBUG] Submissions dataset loaded with {len(self.df_submissions)} rows.\")\n",
    "            \n",
    "#             print(\"[DEBUG] Loading comments dataset...\")\n",
    "#             self.df_comments = pd.read_csv(self.comments_path, sep='\\t', encoding='utf-8', low_memory=True)\n",
    "#             print(f\"[DEBUG] Comments dataset loaded with {len(self.df_comments)} rows.\")\n",
    "            \n",
    "#             print(\"[DEBUG] Loading filtered submissions dataset...\")\n",
    "#             self.df_filtered_submissions = pd.read_csv(self.filtered_submissions_path, encoding='latin1', low_memory=True)\n",
    "#             print(f\"[DEBUG] Filtered submissions dataset loaded with {len(self.df_filtered_submissions)} rows.\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"[ERROR] An error occurred while loading data: {e}\")\n",
    "\n",
    "#     def get_data_by_id(self, data_id):\n",
    "#         try:\n",
    "#             # Debug log for the query\n",
    "#             print(f\"[DEBUG] Searching for ID: {data_id}\")\n",
    "            \n",
    "#             # Search in submissions dataset\n",
    "#             submission_row = self.df_submissions[self.df_submissions['id'] == data_id]\n",
    "#             if not submission_row.empty:\n",
    "#                 print(f\"[DEBUG] Found ID in submissions dataset.\")\n",
    "#             else:\n",
    "#                 print(f\"[DEBUG] ID not found in submissions dataset.\")\n",
    "\n",
    "#             # Search in comments dataset\n",
    "#             comments_row = self.df_comments[\n",
    "#                 (self.df_comments['parent_id'].str.contains(data_id, na=False)) | \n",
    "#                 (self.df_comments['submission_id'] == data_id)\n",
    "#             ]\n",
    "#             if not comments_row.empty:\n",
    "#                 print(f\"[DEBUG] Found ID in comments dataset.\")\n",
    "#             else:\n",
    "#                 print(f\"[DEBUG] ID not found in comments dataset.\")\n",
    "\n",
    "#             # Search in filtered submissions dataset\n",
    "#             filtered_row = self.df_filtered_submissions[self.df_filtered_submissions['id'] == data_id]\n",
    "#             if not filtered_row.empty:\n",
    "#                 print(f\"[DEBUG] Found ID in filtered submissions dataset.\")\n",
    "#             else:\n",
    "#                 print(f\"[DEBUG] ID not found in filtered submissions dataset.\")\n",
    "\n",
    "#             # Log the results\n",
    "#             logging.info(f\"Query ID: {data_id}\")\n",
    "#             logging.info(f\"Submissions data: {submission_row.to_dict(orient='records')}\")\n",
    "#             logging.info(f\"Comments data: {comments_row.to_dict(orient='records')}\")\n",
    "#             logging.info(f\"Filtered submissions data: {filtered_row.to_dict(orient='records')}\")\n",
    "\n",
    "#             # Return all the data\n",
    "#             return {\n",
    "#                 \"submissions\": submission_row,\n",
    "#                 \"comments\": comments_row,\n",
    "#                 \"filtered_submissions\": filtered_row\n",
    "#             }\n",
    "#         except Exception as e:\n",
    "#             print(f\"[ERROR] An error occurred while retrieving data for ID {data_id}: {e}\")\n",
    "#             logging.error(f\"An error occurred for ID {data_id}: {e}\")\n",
    "#             return None\n",
    "\n",
    "# # Main execution\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Define paths\n",
    "#     submissions_path = '/content/drive/MyDrive/TUS/Engineering_Project/data/concatted_all_data.csv'\n",
    "#     comments_path = '/content/drive/MyDrive/TUS/Engineering_Project/data/source_files/all_comments.tsv'\n",
    "#     filtered_submissions_path = '/content/drive/MyDrive/TUS/Engineering_Project/data/cleaned_chunks/merged_cleaned_data_v30_news.csv'\n",
    "\n",
    "#     # Initialize and load datasets into memory\n",
    "#     data_manager = DataInMemory(submissions_path, comments_path, filtered_submissions_path)\n",
    "#     data_manager.load_data()\n",
    "\n",
    "#     # Query data by entering different IDs\n",
    "#     while True:\n",
    "#         data_id = input(\"Enter the ID to search (or type 'exit' to quit): \").strip()\n",
    "#         if data_id.lower() == 'exit':\n",
    "#             print(\"Exiting the program. Goodbye!\")\n",
    "#             break\n",
    "\n",
    "#         result = data_manager.get_data_by_id(data_id)\n",
    "#         if result:\n",
    "#             print(\"\\n[RESULT] Submissions data:\")\n",
    "#             print(result[\"submissions\"])\n",
    "#             print(\"\\n[RESULT] Comments data:\")\n",
    "#             print(result[\"comments\"])\n",
    "#             print(\"\\n[RESULT] Filtered submissions data:\")\n",
    "#             print(result[\"filtered_submissions\"])\n",
    "#         else:\n",
    "#             print(\"[RESULT] No data found for the provided ID.\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

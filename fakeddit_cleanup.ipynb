{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas\n",
    "%pip install nltk\n",
    "%pip install scikit-learn\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load the datasets\n",
    "try:\n",
    "  df_train = pd.read_csv('/content/drive/MyDrive/TUS/Engineering_Project/data/source_files/all_train.tsv', sep='\\t', encoding='latin1', low_memory=True)\n",
    "  df_test = pd.read_csv('/content/drive/MyDrive/TUS/Engineering_Project/data/source_files/all_test_public.tsv', sep='\\t', encoding='latin1', low_memory=True)\n",
    "  df_validate = pd.read_csv('/content/drive/MyDrive/TUS/Engineering_Project/data/source_files/all_validate.tsv', sep='\\t', encoding='latin1', low_memory=True)\n",
    "\n",
    "except Exception as e:\n",
    "  print(f\"An error occurred during data loading: {e}\")\n",
    "\n",
    "try:\n",
    "  df_submissions_clean = pd.concat([df_train, df_test, df_validate], ignore_index=True)\n",
    "except Exception as e:\n",
    "  print(f\"An error occurred during concatenation: {e}\")\n",
    "\n",
    "# Save the cleaned DataFrame to a new CSV file\n",
    "try:\n",
    "    df_submissions_clean.to_csv('/content/drive/MyDrive/TUS/Engineering_Project/data/concatted_all_data.csv', index=False)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred saving to csv: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import glob\n",
    "\n",
    "# Configuration class for cleaning parameters\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.chunk_size = 10000\n",
    "        self.max_text_length = 500\n",
    "        self.comment_invalid_phrases = [\n",
    "            'post comment without original photoshop',\n",
    "            'please post comment photoshop reply',\n",
    "            'unfortunately, your submission has been removed from',\n",
    "            'google image search automated response',\n",
    "            'is a curated space.**',\n",
    "            'In order not to get your comment removed',\n",
    "            'Google cached version',\n",
    "            'http://',\n",
    "            'https://'\n",
    "        ]\n",
    "        # self.stop_words = set(stopwords.words('english'))\n",
    "        self.placeholder = \"placeholderspecialdelim\"\n",
    "\n",
    "# Cleaner class to encapsulate data cleaning\n",
    "class Cleaner:\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        # self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        if isinstance(text, float):  # Handle NaN\n",
    "            print(f\"[DEBUG] Encountered NaN or invalid text: {text}\")\n",
    "            return ''\n",
    "\n",
    "        # Temporarily replace special delimiter\n",
    "        text = text.replace('|__|', self.config.placeholder)\n",
    "        print(f\"[DEBUG] After replacing special delimiter: {text}\")\n",
    "\n",
    "        # Remove unwanted characters\n",
    "        text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "        text = re.sub(r'<.*?>', '', text)  # Remove HTML tags\n",
    "        text = re.sub(r'[^a-zA-Z\\s' + self.config.placeholder + ']', '', text)  # Remove special characters\n",
    "        print(f\"[DEBUG] After cleaning unwanted characters: {text}\")\n",
    "\n",
    "        # Convert text to lowercase\n",
    "        text = text.lower()\n",
    "        print(f\"[DEBUG] Converted text to lowercase: {text}\")\n",
    "\n",
    "        # Truncate text if it exceeds the maximum length\n",
    "        if len(text) > self.config.max_text_length:\n",
    "            print(f\"[DEBUG] Truncating text to max length of {self.config.max_text_length}\")\n",
    "            text = text[:self.config.max_text_length] + '...' + '|__|'\n",
    "\n",
    "        return text\n",
    "\n",
    "    def filter_comments(self, df_comments):\n",
    "        comments_dict = {}\n",
    "        comments_orig_dict = {}\n",
    "\n",
    "        print(f\"[DEBUG] Total comments before filtering: {len(df_comments)}\")\n",
    "\n",
    "        for i, row in df_comments.iterrows():\n",
    "            parent_id = row.get('parent_id', '')\n",
    "            if isinstance(parent_id, str):\n",
    "                parent_id = parent_id.split('_', 1)[-1]  # Remove prefix like 't1_'\n",
    "                print(f\"[DEBUG] Processed parent_id: {parent_id}\")\n",
    "\n",
    "            comment_body = row.get('body', '') if pd.notna(row.get('body', '')) else ''\n",
    "            print(f\"[DEBUG] Original comment body: {comment_body}\")\n",
    "\n",
    "            # Skip invalid comments\n",
    "            if (comment_body in ['NaN', 'deleted', 'removed', '', '[deleted]', '[removed]']) or \\\n",
    "               any(phrase in comment_body.lower() for phrase in self.config.comment_invalid_phrases):\n",
    "                print(f\"[DEBUG] Skipping invalid comment: {comment_body}\")\n",
    "                continue\n",
    "            elif parent_id != '' and comment_body != '':\n",
    "                # Build comment dictionaries\n",
    "                if parent_id in comments_orig_dict:\n",
    "                    # comments_dict[parent_id].append(cleaned_comment)\n",
    "                    comments_orig_dict[parent_id].append(comment_body)\n",
    "                    print(f\"[DEBUG] Appended comment to parent_id {parent_id}\")\n",
    "                else:\n",
    "                    # comments_dict[parent_id] = [cleaned_comment]\n",
    "                    comments_orig_dict[parent_id] = [comment_body]\n",
    "                    print(f\"[DEBUG] Created new entry for parent_id {parent_id}\")\n",
    "\n",
    "        print(f\"[DEBUG] Filtered {len(comments_orig_dict)} unique parent_ids with valid comments.\")\n",
    "        return comments_dict, comments_orig_dict\n",
    "\n",
    "    def clean_dataframe(self, df):\n",
    "        print(f\"[DEBUG] Starting dataframe cleaning. Initial columns: {df.columns}\")\n",
    "\n",
    "        # Remove unnamed columns\n",
    "        unnamed_cols = [col for col in df.columns if col.startswith('Unnamed')]\n",
    "        df.drop(columns=unnamed_cols, inplace=True)\n",
    "        print(f\"[DEBUG] Removed unnamed columns. Remaining columns: {df.columns}\")\n",
    "\n",
    "        # Filter rows based on 'hasImage' and 'subreddit'\n",
    "        initial_row_count = len(df)\n",
    "        df = df[~df['hasImage'].astype(str).str.lower().eq('true')]\n",
    "        print(f\"[DEBUG] Rows after removing hasImage=True: {len(df)} (removed {initial_row_count - len(df)})\")\n",
    "        df = df[~df['subreddit'].str.lower().str.contains('photo')]\n",
    "        print(f\"[DEBUG] Rows after removing subreddits containing 'photo': {len(df)}\")\n",
    "        df = df[df['subreddit'].str.lower().str.contains('news')]\n",
    "        print(f\"[DEBUG] Rows after filtering subreddits containing 'news': {len(df)}\")\n",
    "\n",
    "        # Drop unnecessary columns\n",
    "        drop_columns = ['created_utc', 'domain', 'image_url']\n",
    "        df.drop(columns=drop_columns, inplace=True, errors='ignore')\n",
    "        print(f\"[DEBUG] Dropped columns: {drop_columns}. Remaining columns: {df.columns}\")\n",
    "\n",
    "        return df\n",
    "\n",
    "# Main function to process data in chunks\n",
    "def process_data_in_chunks(config: Config, cleaner: Cleaner, submissions_path, comments_path, output_path):\n",
    "    try:\n",
    "        # Load comments\n",
    "        print(\"[DEBUG] Loading comments dataset...\")\n",
    "        df_comments = pd.read_csv(comments_path, sep='\\t', encoding='utf-8', on_bad_lines='warn', low_memory=True)\n",
    "        print(f\"[DEBUG] Loaded {len(df_comments)} rows from comments dataset.\")\n",
    "\n",
    "        comments_dict, comments_orig_dict = cleaner.filter_comments(df_comments)\n",
    "\n",
    "        # Load submissions in chunks\n",
    "        print(\"[DEBUG] Loading submissions dataset in chunks...\")\n",
    "        chunk_iterator = pd.read_csv(submissions_path, encoding='latin1', low_memory=True, on_bad_lines='skip', chunksize=config.chunk_size)\n",
    "\n",
    "        for chunk_count, chunk in enumerate(chunk_iterator):\n",
    "            print(f\"[DEBUG] Processing chunk {chunk_count + 1} with {len(chunk)} rows...\")\n",
    "            chunk = cleaner.clean_dataframe(chunk)\n",
    "\n",
    "            # Map comments to submissions\n",
    "            print(\"[DEBUG] Mapping comments to submissions...\")\n",
    "            chunk['comments_orig'] = chunk.apply(\n",
    "                # lambda row: '|__|'.join(set(comments_orig_dict.get(row.get('linked_submission_id', ''), []) +\n",
    "                #                            comments_orig_dict.get(row.get('id', ''), []))),\n",
    "                lambda row: '|__|'.join(set(comments_orig_dict.get(row.get('id', ''), []))),\n",
    "                axis=1\n",
    "            )\n",
    "            print(\"[DEBUG] Finished mapping comments for chunk.\")\n",
    "\n",
    "            # Remove rows without comments\n",
    "            initial_chunk_row_count = len(chunk)\n",
    "            chunk = chunk[chunk['comments_orig'] != '']\n",
    "            print(f\"[DEBUG] Rows after filtering out empty comments: {len(chunk)} (removed {initial_chunk_row_count - len(chunk)})\")\n",
    "\n",
    "            # Save cleaned chunk\n",
    "            chunk.to_csv(f\"{output_path}/cleaned_chunk_{chunk_count + 1}_news.csv\", index=False)\n",
    "            print(f\"[DEBUG] Chunk {chunk_count + 1} saved with {len(chunk)} rows.\")\n",
    "\n",
    "        # Combine all cleaned chunks into a single file\n",
    "        all_files = glob.glob(f\"{output_path}/cleaned_chunk_*.csv\")\n",
    "        print(f\"[DEBUG] Combining {len(all_files)} cleaned chunk files...\")\n",
    "        combined_df = pd.concat([pd.read_csv(f) for f in all_files], ignore_index=True)\n",
    "        combined_df.to_csv(f\"{output_path}/merged_cleaned_data_v31_news.csv\", index=False)\n",
    "        print(\"[DEBUG] All chunks merged and saved to merged_cleaned_data.csv.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] An error occurred: {e}\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    config = Config()\n",
    "    cleaner = Cleaner(config)\n",
    "\n",
    "    # Paths\n",
    "    submissions_path = '/content/drive/MyDrive/TUS/Engineering_Project/data/concatted_all_data.csv'\n",
    "    comments_path = '/content/drive/MyDrive/TUS/Engineering_Project/data/source_files/all_comments.tsv'\n",
    "    output_path = '/content/drive/MyDrive/TUS/Engineering_Project/data/cleaned_chunks'\n",
    "\n",
    "    # Process data\n",
    "    process_data_in_chunks(config, cleaner, submissions_path, comments_path, output_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
